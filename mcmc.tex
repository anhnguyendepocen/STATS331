\chapter{Markov Chain Monte Carlo}
\section{Monte Carlo}
Monte Carlo is a general term for computational techniques that use random
numbers.
Monte Carlo can be used in classical and Bayesian statistics, and a special kind
called Markov Chain Monte Carlo (MCMC) was one of the main reasons
for the revival of Bayesian statistics in the second half of the 20th century.
Before MCMC became popular, one of the major drawbacks of the Bayesian approach
was that the calculations were too hard. 

\subsection{Summaries}
So far, we have represented our probability distributions (prior and posterior)
in a computer by using a vector of possible parameter values and a corresponding
vector of probabilities. For example, suppose we had a single parameter $\theta$
and we had worked out the posterior distribution by using a Bayes' Box. This
will have given us a vector {\tt theta} of possible $\theta$ values and a corresponding
vector {\tt post} containing the posterior probabilities. Well, one thing we could
do is plot the posterior distribution:
\begin{verbatim}
plot(theta, post, xlab="Theta", ylab="Posterior Probability")
\end{verbatim}
This would make a plot something like the following:

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/normal.pdf}
\caption{\it A posterior distribution can be represented in a computer by a discrete
set of possible parameter values, and the corresponding probabilities.\label{fig:normal}}
\end{center}
\end{figure}

If we wanted to obtain some summaries, we could do it like so:
\begin{verbatim}
post_mean = sum(theta*post)
post_sd = sqrt(sum(theta^2*post) - post_mean^2)
\end{verbatim}

However,
there is an alternative way of representing this posterior distribution in a
computer. It may not be immediately obvious why this is a good idea,
because there is nothing wrong with the tried and true method we have used
so far. But this second method has the advantage that it continues to
work well on much bigger problems, such as when we have more than one parameter.
With more than one parameter, the `vector of possible solutions'' approach
can fail very dramatically.

Our new way of representing a probability distribution in a computer will be
via {\it Monte Carlo} samples.
Instead of having two vectors (one of $\theta$ values and one of the
corresponding probabilities),
imagine we had some method to compute a random sample of $\theta$ values, drawn
from the posterior distribution in Figure~\ref{fig:normal}.
There would only be one vector {\tt theta}. So how would we
know that there is greater probability around $\theta=1$? Well, {\it more elements
of the {\tt theta} vector would be near 1}.
Instead of carrying around a second vector of
probabilities, we simply let the fact that there are lots of points in certain
regions tell us that those regions are more probable. Say our vector of random
samples was called {\tt theta}. Then we could look at the posterior distribution
by plotting a histogram of samples:
\begin{verbatim}
hist(theta, breaks=100)
\end{verbatim}
The histogram would look something like this:

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/normal2.pdf}
\caption{\it The posterior distribution for a parameter $\theta$ can also be
represented by a random sample of $\theta$ values drawn from the posterior
distribution. The fact that some $\theta$ values are more probable than others
is encoded by the fact that certain values appear more frequently in the sample.
\label{fig:normal2}}
\end{center}
\end{figure}

We could also get our summaries, but the code looks different
(it's actually easier!):
\begin{verbatim}
post_mean = mean(theta)
post_sd = sd(theta)
\end{verbatim}
Because of the randomness involved in generating the {\tt theta} values,
the summaries aren't exact. For example, I know that the actual posterior mean
and standard deviation in this example were both 1, but the values obtained
from the Monte Carlo samples were 0.9604 and 1.0008 respectively. This doesn't
matter much though, because the conclusions about $\theta$ are that it is probably
somewhere around 1 with an uncertainty of about 1, and the error introduced by
using random samples are much smaller than the amount of uncertainty inherent
in the posterior distribution itself. For example, if I summarised the posterior
distribution by saying $\theta = 0.9604 \pm 1.0008$, for almost all practical
purposes the conclusion is exactly the same as the true version of the summaries
$\theta = 1 \pm 1$.

In this discussion we haven't answered the question of how to actually generate
random samples of $\theta$ from the posterior distribution. This is the job
of Markov Chain Monte Carlo.

\begin{framed}
{\bf
The purpose of Markov Chain Monte Carlo is to generate random samples of
parameter values drawn from the posterior distribution. This makes it very easy
to compute summaries even if you have more than one unknown parameter.}
\end{framed}

\section{Multiple Parameters}
MCMC becomes extremely useful when we begin to look at Bayesian models involving
more than one unknown parameter. 
Having posterior samples makes the process of {\it marginalisation}
much easier. Imagine we wanted to infer $a$ and $b$ from
data $x$. Bayes' rule (parameter estimation version) would give us the posterior
distribution:
\begin{eqnarray}
p(a, b | x) \propto p(a, b)p(x|a,b)
\end{eqnarray}

However, what if you didn't really care about the value of $b$ but
only really wanted to measure $a$. The terminology for this is that $b$ is a
{\it nuisance parameter}: you need it to define the model, but ultimately you
are not really interested in knowing its value.
What you need in this case is the
{\it marginal} posterior distribution for $a$ (that is, the posterior
distribution for $a$ on its own, not the joint distribution with $b$).
This can be obtained using the sum rule. The result is:
\begin{eqnarray}
p(a | x) &=& \int_b p(a, b|x) \, db
\end{eqnarray}
or
\begin{eqnarray}
p(a | x) &=& \sum_{b} p(a, b|x)
\end{eqnarray}
depending on whether the possible $b$ values are continuous or discrete.
Before MCMC, these integrals or sums usually couldn't be done without making
certain choices purely for mathematical convenience (e.g. choosing the prior
to be a certain kind of distribution, not because it is a good model of your
prior beliefs, but only because the maths works out neatly).

Samples of parameter values drawn from the posterior distribution
(achieved using MCMC) make this hard problem much easier. We no longer need
to worry about mathematical convenience.
See Figure~\ref{fig:marginalisation} for an example showing how Monte Carlo
sampling makes marginalisation trivial.

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/marginalisation.pdf}
\caption{\it An example of a posterior distribution for two parameters, $a$ and
$b$. The left panels show the joint posterior distribution (which has a
correlation) and the marginal posterior distribution for $a$, obtained by
integrating over all the possible $b$ values. The top right panel contains
random samples
(points) drawn from the posterior distribution for $a$ and $b$.
All you need to do to get the
marginal distribution for $a$ (lower right panel) is ignore the $b$-values of
the points!
\label{fig:marginalisation}}
\end{center}
\end{figure}


\section{The Metropolis Algorithm}
The Metropolis algorithm is the most basic MCMC method.
The ideas behind it are fairly simple, yet Metropolis forms the basis of a large
number of more advanced MCMC methods. In STATS 331 we will study the basic ideas
behind how the Metropolis algorithm works. We will look at a small amount of R code
that implements the Metropolis algorithm, but for solving practical problems
it is more convenient to use the JAGS program\footnote{JAGS uses a number
of MCMC methods internally, including Metropolis, ``Gibbs Sampling'', and
``Slice Sampling'', which we will not study in this course.}.

The Metropolis algorithm was invented in the 1950s by physicists
(including Nicholas Metropolis, for whom the algorithm is named), who used it
to do calculations in the field of statistical mechanics. This intriguing field
focuses on calculating the macroscopic (large scale) properties of matter from
knowledge of the small-scale properties: for example, knowing that water is
H$_2$O, you could use statistical mechanics to figure out that if you have a lot
of water molecules, it will freeze at 0 degrees Celsius and boil at 100 degrees
Celsius.

It took
many decades before people started to realise that the Metropolis algorithm was useful in Bayesian
statistics as well. Even though the Bayesian approached seemed very elegant and
useful to many people, it could always be criticised on the basis that, in order
to implement it on practical problems, you usually had to do difficult or
impossible integrals (to summarise the posterior, or to get rid of nuisance
parameters). MCMC changed all that, and is one of the reasons for the
explosion in the popularity of Bayesian statistics beginning in the 1990s.

\begin{framed}
{\bf
The basic idea of MCMC is that we want a method that will travel between
different possible states (such as the possible hypotheses/parameter values in
a Bayesian analysis), and has the property that the amount of time spent in
any particular state is proportional to the posterior probability of that state.}
\end{framed}

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.65]{Figures/mcmc.pdf}
\caption{\it An illustration of the basic idea behind MCMC. Imagine we had a
Bayes' Box with two possible states, and we knew the
{\tt prior $\times$ likelihood} values. The amount of time the MCMC program will
spend in each state is proportional to the posterior probability of the state.
In this example, the MCMC algorithm was in State 1 three times and in State 2
seven times. Using this, we could estimate the posterior probability of State 2
as being 0.7. This estimate would become more accurate if we ran the MCMC for
more iterations.\label{fig:mcmc}}
\end{center}
\end{figure}

\subsection{Metropolis, Stated}
The Metropolis algorithm is given below. The first thing to do is start
{\it somewhere} in the ``parameter space'' (set of possible parameter values).
You then
{\it propose} to move somewhere else. There is an acceptance probability $\alpha$
that determines whether to accept the proposal. If the proposal is better
($h$, the prior times likelihood value is higher), then you accept it. If the
proposal is worse, you can also accept it, but the probability of accepting is
then $h'/h$ (e.g. if the proposed point is 1/3 as good as the current point, the
acceptance probability is 1/3).

The Metropolis algorithm works by making transitions towards low probability
states rare, but transitions towards high probability states are common, because
of the acceptance probability. It is hard to move into an improbable state, so
not much time will be spent there.

\begin{framed}
\begin{itemize}
\item Start in some state $\theta$
\item Generate a ``proposal'' state $\theta'$ from a {\it proposal distribution}
\item With probability $\alpha = \textnormal{min}(1, h'/h)$, replace the current
state with the proposed state
\item Repeat
\end{itemize}
\end{framed}

\section{A Two State Problem}
We will now study how the Metropolis algorithm would work on a very simple
example, namely the two-ball problem from the beginning of the notes. There
were two hypotheses and the posterior probabilities were $1/3$ and $2/3$.
It is important to note that MCMC is not actually needed for a problem this
simple, but it is a good test case to see precisely how an MCMC algorithm works.
This means we'll discuss a small amount of Markov chain theory.
However, when we solve real data analysis problems with JAGS,
we won't have to think too
much about how the MCMC works.

Let's
call the less probable hypothesis ``State 1'' and the more probable hypothesis
``State 2'' for the purposes of this section.
What we need is a Markov process that will spend 1/3 of the time in State 1 and
2/3 of the time in State 2. The Metropolis algorithm described above will
do what we need. The main thing we need to compute are the acceptance probabilities
$\alpha_{ij}$ for a proposed transition from state $i$ to state $j$ where
$i, j \in \{1, 2\}$.
The acceptance probability $\alpha$ for a proposed move from state $i$ to
state $j$ is given by:
\begin{eqnarray}
\alpha_{ij} = \textnormal{min}\left(1, \frac{h_j}{h_i}\right)
\end{eqnarray}
where $h_i$ and $h_j$ are proportional to the posterior probabilities of
states $i$ and $j$ respectively. This means that if the proposal is to move
to an equal or better (higher posterior probability)
state ($h_j \geq h_i$) then the acceptance probability is 1.
If the proposal is to move to a less probable state then the acceptance probability
is $h_j/h_i$, the ratio of the two posterior probabilities\footnote{Note that
this algorithm can be used even if the marginal likelihood is unknown, because
only ratios of posterior probabilities are needed. This is useful because the
marginal likelihood is sometimes very hard to calculate in multi-parameter
problems.}.

The transition probability is the probability
of being in state $j$ at the next iteration given that you are in state $i$
at the current iteration. The transition probability is given by the product
rule:
\begin{eqnarray}
p_{ij} = q_j \alpha_{ij}\label{eq:transition}
\end{eqnarray}
for $i \neq j$.
The transition matrix of the Markov chain is a matrix with all the different
$p_{ij}$ values in it:
\begin{eqnarray}
\mathbf{P} &=&
\left[
\begin{array}{cc}
p_{11} & p_{12}\\
p_{21} & p_{22}
\end{array}
\right]
\end{eqnarray}
In our particular case, we can work out the off-diagonal elements of
$\mathbf{P}$ using Equation~\ref{eq:transition}:
\begin{eqnarray}
\mathbf{P}
&=&
\left[
\begin{array}{cc}
 & \frac{1}{2} \times 1\\
\frac{1}{2}\times\frac{1}{2} & 
\end{array}
\right]
\end{eqnarray}
The diagonal elements can be found by knowing that the rows of $\mathbf{P}$ must
sum to 1. We must be in {\it some} state at the next iteration. Therefore the
transition matrix of our Markov chain on this two-state problem is:
\begin{eqnarray}
\mathbf{P}
&=&
\left[
\begin{array}{cc}
\frac{1}{2} & \frac{1}{2}\\
\frac{1}{4} & \frac{3}{4}
\end{array}
\right]
\end{eqnarray}

A Markov chain with a small number of possible states can be represented
graphically using a {\it transition diagram}, as in
Figure~\ref{fig:transitions}.

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.65]{Figures/transitions.pdf}
\caption{\it A transition diagram for a Markov Chain with two possible states.
The states (which correspond to hypotheses in Bayesian inference) are drawn
as circles labelled ``1'' and ``2''. When the algorithm is in a particular state
(e.g. State 1) there is a certain probability $p_{11}$ that it will be in State
1 again at the next iteration, and another probability $p_{12}$ that it will
move to State 2. MCMC works by making it easy to move into states with high
posterior probability, and hard to move out of them.
\label{fig:transitions}}
\end{center}
\end{figure}


In class we will do ``Tactile MCMC'', which is an implementation of the
Metropolis algorithm using coins and dice instead of the random number generators
provided in computer software such as R.
