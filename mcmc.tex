\chapter{Markov Chain Monte Carlo}
\section{Monte Carlo}
Monte Carlo is a general term for computational techniques that use random
numbers.
Monte Carlo can be used in classical and Bayesian statistics, and a special kind
called Markov Chain Monte Carlo (MCMC) was partially responsible for the revival
of Bayesian statistics in the second half of the 20th century.

What we have seen so far is that you can represent a probability distribution
in a computer by using a vector of possible values and a corresponding
vector of probabilities. For example, suppose we had a single parameter $\theta$
and we had worked out the posterior distribution by using a Bayes' Box. This
will have given us a vector {\tt theta} of possible $\theta$ values and a corresponding
vector {\tt post} containing the posterior probabilities. Well, one thing we could
do is plot the posterior distribution:
\begin{verbatim}
plot(theta, post, xlab="Theta", ylab="Posterior Probability")
\end{verbatim}
This would make a plot something like the following:
\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/normal.pdf}
\caption{\label{fig:normal}}
\end{center}
\end{figure}
If we wanted to obtain some summaries, we could do it like so:
\begin{verbatim}
post_mean = sum(theta*post)
post_sd = sqrt(sum(theta^2*post) - post_mean^2)
\end{verbatim}

However,
there is an alternative way of representing this posterior distribution in a
computer. It may seem a bit silly at first, because there is nothing wrong with
the above method. But this second method has the advantage that it continues to
work well on much bigger problems, such as when we have more than one parameter.
Instead of having a two vectors: one of $\theta$ values and one of the
corresponding probabilities,
imagine we had some method to compute a random sample of $\theta$ values, drawn
from the posterior distribution. There would only be one vector. So how would we
know that there is greater probability around $\theta=1$? Well, {\it more elements
of the vector would be near 1}. Instead of carrying around a second vector of
probabilities, we simply let the fact that there are lots of points in certain
regions tell us that those regions are more probable! Say our vector of random
samples was called {\tt theta2}. Then we could look at the posterior distribution
by plotting a histogram of samples:
\begin{verbatim}
hist(theta2, breaks=100)
\end{verbatim}
We could also get our summaries, but the code looks different (easier!):
\begin{verbatim}
post_mean = mean(theta2)
post_sd = sd(theta2)
\end{verbatim}






\begin{framed}
{\bf
The purpose of Markov Chain Monte Carlo is to generate random samples of
parameter values drawn from the posterior distribution. This makes it very easy
to compute summaries even if you have more than one unknown parameter.}
\end{framed}


\section{The Metropolis Algorithm}

\section{Tactile MCMC}

Tactile MCMC is an idea due to Wayne Stewart.
