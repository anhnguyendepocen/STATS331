\chapter{Summarising the Posterior Distribution}
The posterior is too much information. We need to summarise it. This is mostly
because, as mere mortals, we might want to communicate our results as a short,
easily remembered ``sound-bite''. For example, say you were trying to estimate
a parameter, and a colleague asked you to state your uncertainty about the
parameter. Well, your posterior distribution might be complicated, it might
have bumps and wiggles in it, or some other kind of structure.
Figure~\ref{fig:complicated_posterior} shows an example of what a complicated
posterior distribution might look like. If this was your result, your colleague
might not care about all the little wiggles in this plot. They just want to know
the ``big picture'' of your results.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.6]{Figures/complicated_posterior.pdf}
\caption{A complicated posterior distribution.\label{fig:complicated_posterior}}
\end{center}
\end{figure}

\begin{framed}
{\bf In descriptive statistics, you often make summaries of a complex data set
(e.g. the mean and the standard deviation) so that you can communicate about
the data set in a concise way. In Bayesian statistics, you often do a similar
thing, but instead of giving a concise description of the data, you give a
concise description of the posterior distribution.}
\end{framed}

\section{Point Estimates}
Point estimates are guesses for the value of the parameter. If you look at the
posterior distribution plotted in Figure~\ref{fig:complicated_posterior}, you
can see that the true value of the parameter is probably somewhere around 5,
but with some uncertainty. If you were to provide a single number as a guess of
the parameter, you would probably say something close to 5. In statistics, a
single number guess is called an ``estimate'', and is written by putting a
little hat over the name of the parameter. So, by looking at the plot of the
posterior, you could give an estimate like this:
\begin{eqnarray}
\hat{\theta} = 5.
\end{eqnarray}
But there are better things you could do, and I'd bet you know some of them
from previous statistics courses. Here are three methods you could use to
choose a point estimate using the posterior distribution: the posterior mean
(expectation value), the posterior median (the value that divides the probability
in half), and the posterior mode (the value where the posterior PDF has its
peak). This gives the following three possible estimates:
\begin{eqnarray}
\hat{\theta} &=& 4.988 \textnormal{(the posterior mean)}\\
\hat{\theta} &=& 4.924 \textnormal{(the posterior median)}\\
\hat{\theta} &=& 4.996 \textnormal{(the posterior mode)}
\end{eqnarray}
In this example, there's not much of a difference between these three methods.
But in other situations, they can be quite different (this usually happens if
the posterior distribution is skewed, or has multiple modes: you may notice a
strong analogy between this topic and descriptive statistics). Is there a way
to say which one is the {\it best}?

\subsection{A Very Brief Introduction to Decision Theory}









\section{Credible Intervals}



