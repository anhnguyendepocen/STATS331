\chapter{A First Example}
We will now look at a simple example that demonstrates all the features of
Bayesian statistics. The problem is quite simple, but we will be able to see
how we start with prior probabilities, and how exactly these get updated
into posterior probabilities after we get more information.


Here we will study a simple example to see what is going on with Bayesian
statistics. Suppose that there are two balls in a bag. We know in advance
that at least one of them is black. But we're not sure whether they're both
black, or whether one is black and one is white.

Consider the two hypotheses:

{\bf BB}: both balls are black\\
{\bf BW}: one is black and one is white.



Show JOINT view

Before we obtain the data, our uncertainties about which hypothesis is true,
and about which data we will observe, can be represented by a {\it joint}
probability distribution.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\hline
\end{tabular}
\end{center}
\end{table}


The Bayes' Box looks like this:
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Possible Hypotheses} & {\tt prior} & {\tt likelihood} &
{\tt prior $\times$ likelihood} & {\tt posterior}\\
\hline
{\tt BB} & 0.5 & 1   & 0.5  & 0.667\\
{\tt BW} & 0.5 & 0.5 & 0.25 & 0.333\\
\hline
Totals: & 1 & & 0.75 & 1\\
\hline
\end{tabular}
\end{center}
\end{table}

