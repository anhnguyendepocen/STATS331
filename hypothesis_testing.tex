\chapter{Hypothesis Testing}
Hypothesis testing is a very important topic that is traditionally considered
as a different topic from parameter estimation. However, in Bayesian statistics
we will see that hypothesis testing is basically the same thing as parameter
estimation! This is a big advantage of Bayesian statistics, that it {\it unifies}
parameter estimation and hypothesis
testing\footnote{``Unifies'' is a popular word for physicists: it means that
two seemingly different topics are fundamentally the same, or very closely
related.}. That's good news, because it means that instead of having to
understand two different topics, we only have to understand one!

To see why hypothesis testing is fundamentally the same, you only need to
understand how parameter estimation works from a Bayesian point of view.
Parameter estimation is nothing more than testing a bunch of hypotheses about
the value of the parameter! e.g. $\theta=1$ vs. $\theta=2$ vs. $\theta=3$ and
so on.

\section{An Example Hypothesis Test}
Suppose we were performing a Bayesian parameter estimation analysis using a
Bayes' Box. Here is an example Bayes' Box with made up numbers:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\tt{possible Values} & \tt{prior} & \tt{pikelihood} & \tt{prior} $\times$ \tt{likelihood} & \tt{posterior}\\
$\theta$ & $p(\theta)$ & $p(x|\theta)$ & $p(\theta)p(x|\theta)$ & $p(\theta|x)$\\
\hline
1.5 & 0.25 & 0.2 & 0.05 & 0.1\\
2.0 & 0.25 & 0.4 & 0.1 & 0.2\\
2.5 & 0.25 & 0.6 & 0.15 & 0.3\\
3.0 & 0.25 & 0.8 & 0.2 & 0.4\\
\hline
Totals & 1 & & 0.5 & 1\\
\hline
\end{tabular}
\end{center}
\end{table}
Suppose we wanted to test the following two hypotheses about the parameter $\theta$:
\begin{eqnarray}
H_0: && \theta = 2\\
H_1: && \theta \neq 2
\end{eqnarray}
In classical statistics, you would need to come up with a {\it test statistic},
and then calculate a {\it p-value} which tries to say something about whether
the value of the test statistic would be considered extreme, under the
assumption that $H_0$ is true.

In Bayesian statistics, as we have seen, the only thing we need to do is calculate
the posterior probability of $H_0$ and the posterior probability of $H_1$.
The posterior probability of $H_0$ is given by:
\begin{eqnarray}
P(H_0|x) &=& P(\theta = 2|x)\\
&=& 0.2
\end{eqnarray}
All we did here was look up the appropriate number in the Bayes' Box! The
posterior probability of $H_1$ is only slightly harder to calculate: $H_1$ will
be true if $\theta$ takes any value other than 2. Therefore:
\begin{eqnarray}
P(H_0|x) &=& P(\theta = 2|x)\\
&=& 0.2
\end{eqnarray}
Note that we didn't use Bayes' rule here, but remember that Bayes' rule is
what is used to calculate the posterior probabilities in the Bayes' box. In this
example, the Bayes' rule calculations were already done for us.
\begin{eqnarray}
P(H_1|x) &=& P(\theta = 1.5 \textbf{ or } \theta = 2.5 \textbf{ or } \theta = 3|x)\\
&=& P(\theta = 1.5|x) + P(\theta = 2.5|x) + P(\theta = 3|x)\\
&=& 0.1 + 0.3 + 0.4\\
&=& 0.8.
\end{eqnarray}
Here we used the fact that everything in a Bayes' Box is mutually exclusive
(only one of the hypotheses is true) so we could add the probabilities.
Alternatively, you could have just noticed that $H_1$ is true if $H_0$ is false!
So $P(H_0|x) + P(H_1|x) = 1$ which implies $P(H_1|x) = 1 - P(H_0|x)$.

\section{The Testing Prior}

