\chapter{Regression}
Regression is a very important topic in statistics that is applied extremely
frequently. There are many different kinds of regression, but in STATS 331 we will
mostly focus on linear regression. This gives us a nice familiar example
example to demonstrate how Bayesian statistics works and how it is different
from classical or frequentist statistics. Here we will study an example of a
simple linear regression problem taken from STATS 20X.

\section{A Simple Linear Regression Problem}
Data were collected from a sample of 30 drivers. The age of the driver and the 
maximum distance at which they could read a newly designed road sign were 
recorded. It is of interest to build a simple model that can be used to predict the 
maximum distance at which the sign is legible, using the age of the driver.
Figure~\ref{fig:road} shows the data.
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{Figures/road.pdf}
\caption{The distance at which a person can read a road sign, vs. the age of
the person. There are $N=30$ data points. You can clearly see that older people
have, on average, worse eyesight. Simple linear regression can be thought of as
``fitting a straight line'' to the data.
\label{fig:road}}
\end{center}
\end{figure}
The purpose of simple linear regression is to find a straight line that goes
throught the data points. The slope and intercept of the straight line are then
helpful for understanding what is going on. Also, the straight line can be used
to predict future data, such as the maximum distance for reading the sign for a
person who is 100 years old. The most common method for obtaining the straight
line is to find the line (i.e. the slope and intercept values) that fits best
by the criterion of ``least squares''.

\section{Interpretation as a Bayesian Question}
From what you now know about Bayesian statistics, you might be able to come up
with some thoughts for what is unsatisfactory about the standard least squares
solution to linear regression. One glaring issue is that the data are hardly ever
going to be good enoughto tell us with certainty that a particular slope and intercept
are correct (the exception would be if three or more points laid perfectly on a straight
line, with no scatter).
In principle, we will almost always have uncertainty about the slope and
the intercept. From a Bayesian perspective, our goal is not to find a point
estimate for the slope and the intercept. Instead we should calculate the
{\it posterior distribution for the slope and the intercept, given the data}.
The posterior distribution will tell us exactly how much uncertainty we have.
If we do want, summaries for convenience, we can use the posterior distribution
to create the summaries, as discussed in previous chapters.

The equation for a straight line is usually written as $y = mx + b$ where $m$
is the gradient/slope and $b$ is the intercept. However,
for consistency with later, more complex regression models, we will write the
equation as:
\begin{eqnarray}
y = \beta_0 + \beta_1 x.
\end{eqnarray}
Here, $\beta_0$ is the $y$-intercept and $\beta_1$ is the slope. Our goal is
to calculate the posterior distribution for $\beta_0$ and $\beta_1$ given the
data.

\section{Analytical Solution With Known Variance}
Bayes' rule (parameter estimation version) tells us how to calculate the
posterior distribution:
\begin{eqnarray}
p(\theta|x) \propto p(\theta)p(x|\theta)
\end{eqnarray}
This is the generic form for parameters $\theta$ and data
$x$. In our particular case, the
unknown parameters are $\beta_0$ and $\beta_1$, and the data are the
$y$ values of the data points. The data also consist of a number $N$ of points,
and the $x$-values,
but we shall assume that these on their own provide no information about the
slope and intercept (it would be a bit strange if they did). So the $x$-values
and the number of points $N$ act like prior information that lurks ``in the
background'' of this entire analysis. The $y$-values are our data in the sense
that we will obtain our likelihood by writing down a probability distribution
for the $y$-values given the parameters.

Therefore, Bayes' rule {\it for this problem} (i.e. with the actual names
of our parameters and data, rather than generic names) reads:
\begin{eqnarray}
p(\beta_0, \beta_1 | y_1, y_2, ..., y_N) \propto
p(\beta_0, \beta_1)p(y_1, y_2, ..., y_N | \beta_0, \beta_1)
\end{eqnarray}

We can now say some things about Bayesian linear regression by working
analytically. For starters, 
let's assume uniform priors for both $\beta_0$ and $\beta_1$, and that the
prior for these two parameters are independent. The probability density for
a uniform prior distribution can be written simply as:
\begin{eqnarray}
p(\beta_0, \beta_1) \propto 1.
\end{eqnarray}
Note that we have written proportional instead of equals. If we decided to
place the limits at -500 and 500 (say) then the actual value of the density
would be $10^{-6}$. But this is just a number and in the end, when we normalise
the posterior distribution it won't matter. We can even imagine making our
prior ``infinitely wide'', which is called an improper prior. In many cases
simply writing $p(\beta_0, \beta_1) \propto 1$ will not cause any problems. We
are assuming the prior probability density is uniform over a very wide range
that we will not specify.

Now, on to the likelihood. There are $N$ data points and so there are $N$
$y$-values in the dataset, called $\{y_1, y_2, ..., y_N\}$. We can obtain the
likelihood by writing down a probability distribution for the data given the
parameters, sometimes called a ``sampling distribution''. This describes our
beliefs about the connection between the data and the parameters, without which
it would be impossible to learn anything from data.
If we
knew the true values of $\beta_0$ and $\beta_1$, then we would predict the
$y$-values to be scattered around the straight line. Specifically we
will assume that each point departs from the straight line by an amount
$\epsilon_i$ which has a $\mathcal{N}(0, \sigma^2)$ probability distribution.
For now, we will assume $\sigma$, the standard deviation of the scatter, is known.

In ``$\sim$'' notation, this can be written as:
\begin{eqnarray}
y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2).
\end{eqnarray}
It is implied that all of the data values are independent (given the
parameters). Therefore the likelihood can be written as a product of $N$
normal densities, one for each data point:
\begin{eqnarray}
p(\{y_1, y_2, ..., y_N\}|\beta_0, \beta_1) &=& \prod_{i=1}^N \frac{1}{\sigma\sqrt{2\pi}}
\exp\left[-\frac{1}{2\sigma^2}\left(y_i - (\beta_0 + \beta_1 x_i)\right)^2\right].
\end{eqnarray}
Remember that when we combine the likelihood with the prior using Bayes' rule,
we can usually ignore any constant factors out the front that do not depend on
the parameters. This allows us to ignore the first part of the product, outside
the exponential.
\begin{eqnarray}
p(\beta_0, \beta_1 | y_1, y_2, ..., y_N) &\propto& p(\beta_0, \beta_1)
p(y_1, y_2, ..., y_N|\beta_0, \beta_1)\\
&\propto& 1 \times \prod_{i=1}^N \exp\left[-\frac{1}{2\sigma^2}\left(y_i - (\beta_0 + \beta_1 x_i)\right)^2\right]\\
&\propto& \exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^N\left(y_i - (\beta_0 + \beta_1 x_i)\right)^2\right].
\label{eq:leastsquares}
\end{eqnarray}
We have just found the expression for the posterior distribution for $\beta_0$
and $\beta_1$. This is a distribution for two parameters (i.e. it is bivariate).
It is not easy to interpret this equation just by
looking at it, but we could use it to work out the value of the posterior
probability density for any possible values of $\beta_0$ and $\beta_1$

There are a few things you may notice about the posterior distribution in
Equation~\ref{eq:leastsquares}. Firstly, the way it depends on the parameters
is an exponential of something involving $\beta_0$
and $\beta_1$ in linear and second-order ways (if you were to expand the square,
you would get terms like $\beta_0\beta_1$ and $\beta_0^2$). Mathematicians would
call the expression inside the exponential a {\it quadratic form}. When a
probability density can be written as the exponential of a quadratic form, it
is a normal density. Therefore the posterior distribution for $\beta_0$ and
$\beta_1$ is a (bivariate) normal distribution.

We can obtain some more insight about this problem by inspecting the
sum term inside the exponential in the posterior distribution
(Equation~\ref{eq:leastsquares}). The sum is over all the data points, and
what is being summed is the difference between the data value $y_i$ and the
straight line prediction $\beta_0 + \beta_1 x_i$, all squared.
In other words, the sum term is nothing more than the sum of squared
residuals that is minimised when solving this problem by ``least squares''.
In classical least squares linear regression, $\beta_0$ and $\beta_1$ are
estimated by minimising this sum of squared residuals. Because of the exp and
the minus sign, the posterior distribution is telling us that the choice of
$\beta_0$ and $\beta_1$ that minimises the sum of squared residuals, maximises
the posterior probability density. Other values of the parameters that
don't quite minimise the sum of squared residuals are somewhat plausible, and
the form of the posterior density tells us exactly how much less plausible they
are. The take home message is summarised below.

\begin{framed}
{\bf Doing a linear regression by least squares is equivalent to having a
uniform prior and a normal likelihood, and finding the posterior mode.
If you think this is appropriate in a particular application, and you are happy
with just a point estimate, then classical
least squares fitting will be fine. Otherwise, you'd better do a Bayesian analysis.}
\end{framed}

While classical regression results may come with ``standard errors'', these are
not the same as a posterior distribution. A posterior distribution describes the
uncertainty about the parameters, given the specific data set you actually have.
Standard errors describe how different your point estimate would be if your data
set was different.

\section{Solution With JAGS}
The above results made one major unrealistic assumption: that the standard
deviation $\sigma$, or scatter, was known. In practice, it usually needs to be
estimated from the data as well. Therefore, in the Bayesian framework, we should
have it be an unknown parameter. Now we have three unknown parameters instead
of two: these are the intercept of the straight line, the gradient of
the straight line, and the standard deviation of the noise or scatter.
\begin{eqnarray}
\boldsymbol{\theta} = \{\beta_0, \beta_1, \sigma\}.
\end{eqnarray}
One major advantage of MCMC is that we can increase the number of unknown parameters
without having to worry about the fact that the posterior distribution might
be hard to interpret or plot.

The data is the same as before:
\begin{eqnarray}
\boldsymbol{D} = \{y_1, y_2, ..., y_N\}
\end{eqnarray}
and so is the likelihood:
\begin{eqnarray}
y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2).
\end{eqnarray}

Our three parameters will need priors. JAGS requires proper priors (i.e. we
can't have a uniform prior over an infinite range), so we will have to be a
little careful about that. Instead of using uniform distributions this time,
we will use normal distributions with a mean of 0 and a large standard deviation
of 1000.

For the standard deviation parameter $\sigma$, we know firstly that this cannot
be negative. Let's use a log-uniform prior with generous lower and upper limits,
so we express uncertainty about the order of magnitude of $\sigma$.
In JAGS, the model looks like this:
\begin{framed}
\begin{verbatim}
model
{
    # Prior for all the parameters
    beta0 ~ dnorm(0, pow(1E3, -2))
    beta1 ~ dnorm(0, pow(1E3, -2))
    log_sigma ~ dunif(-10, 10)
    sigma <- exp(log_sigma)

    # Likelihood
    for(i in 1:N)
    {
        y[i] ~ dnorm(beta0 + beta1*x[i], pow(sigma, -2))
    }
}
\end{verbatim}
\end{framed}
The first part defines the priors for the parameters. For $\beta_0$ and $\beta_1$,
we have just chosen very broad priors that describe vague prior knowledge. Note
that the standard deviations of the priors are 1000, so we should be careful
to only apply this code to situtations where we don't expect the intercept or
slope to take on an extreme value.

For the standard deviation parameter $\sigma$ which describes how much we
expect the data points to be scattered around the straight line, we have assigned
a log-uniform prior. The limits of $-10$ and $10$ for {\tt log\_sigma} imply
limits of $4.5 \times 10^{-5}$ to 22,000 for {\tt sigma}, a generous range.
Again, if we thought the scatter
was going to be outside this range, we should change the prior to something
else or risk getting strange answers.

So we have the code - now all we have to do is run it!

\section{Results for ``Road'' Data}
Our JAGS output will contain samples from the posterior distribution for
$\beta_0$, $\beta_1$ and $\sigma$. The first thing we should to is make
trace plots and check that everything converged properly. Then we can make
histograms of each parameter to visually inspect the (marginal) posterior
distribution for each parameter.
\begin{framed}
\begin{verbatim}
# Plot trace plots
plot(results$beta0, type='l', xlab='Iteration', ylab='beta0')
plot(results$beta1, type='l', xlab='Iteration', ylab='beta1')
plot(results$sigma, type='l', xlab='Iteration', ylab='sigma')

# Plot histograms
hist(results$beta0, breaks=20, xlab='beta0')
hist(results$beta1, breaks=20, xlab='beta1')
hist(results$sigma, breaks=20, xlab='sigma')

# Plot joint posterior distribution of beta0 and beta1
plot(results$beta0, results$beta1, cex=0.1, xlab='beta0', ylab='beta1')
\end{verbatim}
\end{framed}
All of the plots for the road data are shown below, in Figure~\ref{fig:road_results}.
\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/road_trace.pdf}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/road_hist.pdf}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/road_joint.pdf}
\end{center}
\end{figure}


\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/road_lines.pdf}
\end{center}
\end{figure}

Everything checks out (it's quite rare to see such perfect trace plots!), so
we can use the samples to make any posterior summaries we might want.

\section{Predicting New Data}
One of the most important uses of regression models is for prediction.

We can do the prediction in R by taking the posterior samples for the parameters,
computing the prediction based on each posterior sample, and then combining
the results from each sample. Alternatively, an extra line can be added to the
JAGS model. Since this extra variable is not observed (it's not data: we didn't
give it a fixed value) it will just get sampled from its distribution as the
MCMC goes along. These two approaches will give equivalent results. Which one
is more convenient depends mostly on how long the MCMC takes. If it takes a long
time, you may not want to run it again, in which case basing the prediction on
existing samples is probably the best idea. If it's no problem to re-run the
MCMC, it is less programming work to add the extra variable in JAGS (don't forget
to monitor it!).

Let's predict the maximum sign-reading distance for a 90-year old. Classically,
you could take the best-fit line and extrapolate it to $x=90$ to obtain an
estimate. That works here too, but we need to consider some changes to this
procedure. Firstly, note that the estimate based on simply extrapolating a
single line is just a number. We are being Bayesian, so we do not want a single
number ``guess'' as our answer: we want a probability distribution! Notice that
{\it if} we knew the true values of the parameters, we could simply put a normal
distribution around what the true straight line predicts at $x=90$. But we
{\it don't know} the true values, we only have the posterior distribution. What
probability theory tells us to do is to in this situation is to make a {\it
mixture}: that is, take the normal distribution you'd get for each possible
value of the parameters, and add them all up, weighted by their posterior
probability. This gives a distribution that is wider than what you'd get by
using a single guess for the parameters. While the distribution is wider, it
is more honest, because it takes into account all the relevant uncertainty.

In JAGS, the prediction can be implemented by adding this single extra line:
\begin{framed}
\begin{verbatim}
y_new ~ dnorm(beta0 + beta1*90, pow(sigma, -2))
\end{verbatim}
\end{framed}
and then monitoring {\tt y\_new} during the run. The samples of {\tt y\_new}
will be drawn from the posterior predictive distribution for the new data.
Note that when predicting new data, the extra line(s) you'll add to the JAGS
model usually resemble the likelihood. The predictive distribution is shown in
Figure~\ref{fig:road_prediction}.
\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.5]{Figures/road_prediction.pdf}
\caption{Samples from the posterior predictive distribution, answering the
question ``what do we know about $y$ at $x=90$?''. Summaries are shown in
the title. If we simply assumed the best fit line was true and applied a
point estimate of $\sigma$ to get our uncertainty, we would have obtained a
prediction of $306.07 \pm 49.76$.}
\end{center}
\end{figure}

\section{Simple Linear Regression With Outliers}
One complication that is common in linear regression (or other model-fitting)
problems is the existence of outliers. These are points that do not fit in with
the general trend. If these are left in the analysis, the results can easily
be incorrect. Many methods exist for deciding how to ``detect'' and ``remove''
outliers. In lectures and labs we will study and extension to the simple linear
regression model that allows for outliers. Bayesian statistics is capable of
handling outliers well, as long as we build our model in such a way that it says
outliers are possible: i.e. we reject the simple normal distribution likelihood
because it doesn't have heavy enough tails.
Rather than simply ``rejecting'' some
points as outliers, we can leave them in, but there will be a posterior probability
that each point is an outlier! That way, all points can contribute to the results.
Information is not wasted.

\section{Multiple Linear Regression and Logistic Regression}
We will study an example of multiple linear regression in lectures.

We will not study a classic example of logistic regression, but the special
lecture on predicting sports matches will be very closely related to logistic
regression! The main difference here is that the output variables are binary
(zeroes and ones) instead of continuous.
